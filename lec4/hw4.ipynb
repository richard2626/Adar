{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Rdqs5EAyr0v3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zOM-K0Xpr3kC"
   },
   "outputs": [],
   "source": [
    "# Check device\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available(): device = torch.device('cuda')\n",
    "    else: device = torch.device('cpu')\n",
    "    # print(device)\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9FIHHy2Kr8rZ"
   },
   "outputs": [],
   "source": [
    "# Data transform and augmentation\n",
    "def data_trans_and_aug():\n",
    "    # train_tfms = transforms.Compose([   transforms.RandomCrop(32, padding=4),\n",
    "    #                                     transforms.RandomHorizontalFlip(),\n",
    "    #                                     transforms.ToTensor(),\n",
    "    #                                     transforms.Normalize( mean = (0.4914, 0.4822, 0.4465),\n",
    "    #                                                           std = (0.247, 0.243, 0.261))])\n",
    "    train_tfms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # 使用單通道的正規化\n",
    "    ])\n",
    "    valid_tfms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # 使用單通道的正規化\n",
    "    ])\n",
    "\n",
    "\n",
    "    # valid_tfms = transforms.Compose([   transforms.ToTensor(),\n",
    "    #                                     transforms.Normalize( mean = (0.4914, 0.4822, 0.4465),\n",
    "    #                                                           std = (0.247, 0.243, 0.261))])\n",
    "    return train_tfms, valid_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JB-dX_oOr-ba"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "def Test(model, device, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            out = model(images)\n",
    "            val_loss += F.cross_entropy(out, labels).item()\n",
    "            pred = out.argmax(dim=1, keepdim=True)\n",
    "            val_acc += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = (100. * val_acc) / len(val_loader.dataset)\n",
    "    print('\\nTest Set: Average loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
    "    return val_acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-tCU0cK9sAjf"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "def Train(epochs, lr, model, device, train_loader, val_loader, save_path, resume):\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    # training setting\n",
    "    best_acc = 0.\n",
    "    start_epoch = 1\n",
    "    results = {}\n",
    "    results['train_loss'] = []\n",
    "    results['train_accu'] = []\n",
    "    results['valid_loss'] = []\n",
    "    results['valid_accu'] = []\n",
    "    # training resume\n",
    "    if resume:\n",
    "        assert os.path.isfile(resume)\n",
    "        checkpoint = torch.load(resume)\n",
    "        print('Loading checkpoint model...')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print('Loading checkpoint optimizer...')\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_acc = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print('Start at epoch', start_epoch, 'with the best accuracy: ' + str(best_acc) + ' %')\n",
    "\n",
    "    # Start training\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        model.train()\n",
    "        print('\\n')\n",
    "        epoch_loss = 0.\n",
    "        train_acc = 0\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(images)\n",
    "            pred = out.argmax(dim=1, keepdim=True)\n",
    "            train_acc += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            loss = F.cross_entropy(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(images), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "        train_acc = (100. * train_acc) / len(train_loader.dataset)\n",
    "        results['train_loss'].append(epoch_loss/len(train_loader))\n",
    "        results['train_accu'].append(train_acc)\n",
    "        # Validataion\n",
    "        val_acc, val_loss = Test(model, device, val_loader)\n",
    "        results['valid_loss'].append(val_loss)\n",
    "        results['valid_accu'].append(val_acc)\n",
    "        # Save Checkpoint\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print('Saving checkpoint...')\n",
    "            state = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'best_acc': best_acc }\n",
    "            if not os.path.isdir(save_path): os.mkdir(save_path)\n",
    "            torch.save(state, save_path + 'checkpoint.pth')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tqcySZ1Au87S"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_acc_and_lr\u001b[39m(results):\n\u001b[1;32m      3\u001b[0m   train_loss \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_acc_and_lr(results):\n",
    "  train_loss = results['train_loss']\n",
    "  val_loss = results['valid_loss']\n",
    "  plt.subplot(2,1,1)\n",
    "  plt.plot(train_loss, \"-o\", label='train_loss')\n",
    "  plt.plot(val_loss, \"-x\", label='val_loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('loss')\n",
    "  plt.legend()\n",
    "  plt.title('Loss vs. Number of epochs')\n",
    "\n",
    "  train_acc = results['train_accu']\n",
    "  val_acc = results['valid_accu']\n",
    "  plt.subplot(2,1,2)\n",
    "  plt.plot(train_acc, \"-o\", label='train_acc')\n",
    "  plt.plot(val_acc, \"-x\", label='val_acc')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.legend()\n",
    "  plt.title('Accuracy vs. Number of epochs')\n",
    "\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjD2sJidu_3Z"
   },
   "outputs": [],
   "source": [
    "def testing_for_performance(test_dl, model, device):\n",
    "  model.eval()\n",
    "  batch_pred_prob = []\n",
    "  batch_pred_label = []\n",
    "  batch_label = []\n",
    "  with torch.no_grad():\n",
    "    for images, labels in test_dl:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      outputs = model(images)\n",
    "      pred_prob, pred_label = F.softmax(outputs, dim=1).max(1)\n",
    "      batch_pred_prob.append(pred_prob.cpu())\n",
    "      batch_pred_label.append(pred_label.cpu())\n",
    "      batch_label.append(labels.cpu())\n",
    "    return torch.cat(batch_label).numpy(), torch.cat(batch_pred_label).numpy()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "def performance(model, test_dl, device):\n",
    "  labels, preds = testing_for_performance(test_dl, model, device)\n",
    "  cm = metrics.confusion_matrix(labels, preds)\n",
    "  accuracy = metrics.accuracy_score(labels, preds)\n",
    "  precision = metrics.precision_score(labels, preds, average='macro')\n",
    "  recall = metrics.recall_score(labels, preds, average='macro')\n",
    "  F1_score = metrics.f1_score(labels, preds, average='macro')\n",
    "\n",
    "  print(\"Accuracy:\",accuracy)\n",
    "  print(\"Precision:\",precision)\n",
    "  print(\"Recall:\",recall)\n",
    "  print('F1_score:',F1_score)\n",
    "  cm_plot = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "  cm_plot.plot()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLiJB-7_wMLX",
    "outputId": "faa68cd8-90c5-42d7-a154-510b9870bf43"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights, resnet50\n",
    "Net = resnet18(pretrained=True)\n",
    "\n",
    "Net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "\n",
    "num_ftrs = Net.fc.in_features\n",
    "Net.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "\n",
    "for param in Net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "flatten_feature = Net.fc.in_features\n",
    "Net.fc = nn.Linear(flatten_feature, 10)\n",
    "\n",
    "for name, param in Net.named_parameters():\n",
    "    print(f'Parameter: {name}, Requires Grad: {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y6wvTdmFrxoE",
    "outputId": "bdbd36cd-0f49-481a-a9aa-b9c6d77ebfd0"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # saving\n",
    "    save_path = 'checkpoint/'\n",
    "\n",
    "    # parser: add/modify any argument if you need\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='number of epochs')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='batch size')\n",
    "    parser.add_argument('--resume', default='', help='resume training from checkpoint')\n",
    "    opt = parser.parse_args(\n",
    "        args=[\n",
    "            '--epochs', '10',\n",
    "            '--lr', '1e-4',\n",
    "            '--batch_size', '16',\n",
    "            '--resume', ''\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # check if GPU is available\n",
    "    device = get_default_device()\n",
    "\n",
    "    # transform and augmentation\n",
    "    train_tfms, valid_tfms = data_trans_and_aug()\n",
    "\n",
    "    # training / validation datasets\n",
    "    train_ds = datasets.MNIST('./data', train=True, transform=train_tfms, download=True)\n",
    "    valid_ds = datasets.MNIST('./data', train=False, transform=valid_tfms, download=True)\n",
    "\n",
    "    # data loaders\n",
    "    train_dl = DataLoader(train_ds, batch_size=opt.batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=opt.batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "    # model\n",
    "    model = Net.to(device)\n",
    "\n",
    "    # Start training\n",
    "    results = Train(opt.epochs, opt.lr, model, device, train_dl, valid_dl, save_path, opt.resume)\n",
    "\n",
    "    # Finish\n",
    "    print(\"\\nTraining Finish\")\n",
    "\n",
    "    # Analysis\n",
    "    performance(model, valid_dl, device)\n",
    "    plot_acc_and_lr(results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nw-I_tnxGFab"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
