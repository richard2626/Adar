{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DZ2OXn9VEd5U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import RandomErasing\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnHhtrafFAWY"
   },
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BoGB9k5aExmk"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomErasing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m valid_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m  \u001b[38;5;66;03m# DO NOT MODIFY !!!\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# convert data to torch.FloatTensor\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_tfms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),  \u001b[38;5;66;03m# 随机水平翻转\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomRotation(\u001b[38;5;241m10\u001b[39m),      \u001b[38;5;66;03m# 随机旋转10度\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(\u001b[38;5;241m28\u001b[39m, scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)),  \u001b[38;5;66;03m# 随机缩放和裁剪\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomGrayscale(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m),  \u001b[38;5;66;03m# 随机灰度变换\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),              \u001b[38;5;66;03m# 转换为张量\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.1\u001b[39m,), (\u001b[38;5;241m0.1\u001b[39m,)),  \u001b[38;5;66;03m# 标准化\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mRandomErasing\u001b[49m(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.02\u001b[39m, \u001b[38;5;241m0.33\u001b[39m), ratio\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m3.3\u001b[39m), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Random Erasing\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ])\n\u001b[1;32m     19\u001b[0m valid_tfms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     20\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     21\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.1\u001b[39m,), (\u001b[38;5;241m0.1\u001b[39m,))\n\u001b[1;32m     22\u001b[0m ])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# choose the training and test datasets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomErasing' is not defined"
     ]
    }
   ],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 50\n",
    "# proportion of validation set to training set\n",
    "valid_size = 0.2  # DO NOT MODIFY !!!\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "    transforms.RandomRotation(10),      # 随机旋转10度\n",
    "    transforms.RandomResizedCrop(28, scale=(0.8, 1.0)),  # 随机缩放和裁剪\n",
    "    transforms.RandomGrayscale(p=0.1),  # 随机灰度变换\n",
    "    transforms.ToTensor(),              # 转换为张量\n",
    "    transforms.Normalize((0.1,), (0.1,)),  # 标准化\n",
    "    RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0)  # Random Erasing\n",
    "])\n",
    "\n",
    "valid_tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1,), (0.1,))\n",
    "])\n",
    "\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.FashionMNIST(root='data', train=True, download=True, transform=train_tfms)\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=valid_tfms)\n",
    "\n",
    "# split the training dataset into training set and validation set\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:],indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "    num_workers=num_workers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us8yEHBNFJ4m"
   },
   "source": [
    "# FashionMNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "V1vynrWtFGse",
    "outputId": "e3f7f894-d520-413e-efc2-265dac3f6be2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# obtain one batch of training images\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z96Lkt8jFcSE"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeOoA2cXFbSe",
    "outputId": "6ec6ee50-f9ab-4054-84e3-05606575192f"
   },
   "outputs": [],
   "source": [
    "## define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # linear layers\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # DO NOT MODIFY !!!\n",
    "        self.fc2 = nn.Linear(512, 256)    # DO NOT MODIFY !!!\n",
    "        self.fc3 = nn.Linear(256, 10)    # DO NOT MODIFY !!!\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)      # DO NOT MODIFY !!!\n",
    "        x = self.fc1(x)           # DO NOT MODIFY !!!\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.fc2(x)           # DO NOT MODIFY !!!\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.fc3(x)           # DO NOT MODIFY !!!\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWFddai8Fq7b"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qVsh1VNFg-d"
   },
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 10  # DO NOT MODIFY !!!\n",
    "\n",
    "# specify optimizer\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmXdLudYF0I5"
   },
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLgg33p4FvFu",
    "outputId": "505bff41-e3f8-4926-aeb8-8c455cf34f29"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to GPU\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "# Initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "# Initialize history for recording what we want to know\n",
    "history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Monitor training loss, validation loss and learning rate\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    lrs = []\n",
    "    result = {'train_loss': [], 'val_loss': [], 'lrs': []}\n",
    "\n",
    "    # Prepare model for training\n",
    "    model.train()\n",
    "\n",
    "    #######################\n",
    "    # Train the model #\n",
    "    #######################\n",
    "    for data, target in train_loader:\n",
    "        # Move data and target to GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # Perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # scheduler.step(valid_loss)\n",
    "\n",
    "        # Record learning rate\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # Update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "    ######################\n",
    "    # Validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # Turn off gradients for validation to save memory and computations\n",
    "        for data, target in valid_loader:\n",
    "            # Move data and target to GPU\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # Update running validation loss\n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "    # Print training/validation statistics\n",
    "    # Calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    result['train_loss'] = train_loss\n",
    "    valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "    result['val_loss'] = valid_loss\n",
    "    leaning_rate = lrs\n",
    "    result['lrs'] = leaning_rate\n",
    "    history.append(result)\n",
    "    # scheduler.step()\n",
    "\n",
    "    print('Epoch {:2d}: Learning Rate: {:.6f} Training Loss: {:.6f} Validation Loss: {:.6f}'.format(\n",
    "        epoch + 1,\n",
    "        leaning_rate[-1],\n",
    "        train_loss,\n",
    "        valid_loss\n",
    "    ))\n",
    "    # scheduler.step(valid_loss)\n",
    "    # Save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print(\"Validation loss decreased ({:.6f} --> {:.6f}). Saving model ..\".format(\n",
    "            valid_loss_min,\n",
    "            valid_loss\n",
    "        ))\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        valid_loss_min = valid_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSanr_ywGDXB"
   },
   "source": [
    "# Plot Learning Rate Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "67JAJnQNF6bu",
    "outputId": "3b5265b1-33a9-4359-feb9-14119e9b915a"
   },
   "outputs": [],
   "source": [
    "def plot_lrs(history):\n",
    "  lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
    "  plt.plot(lrs)\n",
    "  plt.xlabel('Batch no.')\n",
    "  plt.ylabel('Learning rate')\n",
    "  plt.title('Learning Rate vs. Batch no.');\n",
    "\n",
    "plot_lrs(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEeZqHJhGQqu"
   },
   "source": [
    "# Plot Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "jxPbvv6FGOCz",
    "outputId": "65a43560-caad-411a-df44-3f1293d5d654"
   },
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "  train_losses = [x.get('train_loss') for x in history]\n",
    "  val_losses = [x['val_loss'] for x in history]\n",
    "  plt.plot(train_losses, '-bx')\n",
    "  plt.plot(val_losses, '-rx')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('loss')\n",
    "  plt.legend(['Training', 'Validation'])\n",
    "  plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li2VnIiGGZPd"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6Tatc0UGVzi",
    "outputId": "7e2f2626-fa20-4147-fbec-79fe7e6c92fa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 将模型移动到GPU\n",
    "model.to(device)\n",
    "\n",
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# prep model for evaluation\n",
    "model.eval()\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of Class %5s: %2.2f%% (%2d/%2d)' % (\n",
    "            str(i),\n",
    "            100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]),\n",
    "            np.sum(class_total[i])\n",
    "            ))\n",
    "    else:\n",
    "        print('Test Accuracy of Class %5s: N/A (no training examples)' % (str[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2.2f%% (%2d/%2d)' % (\n",
    "    100 * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct),\n",
    "    np.sum(class_total)\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
